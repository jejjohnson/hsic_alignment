{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Information Theory Measures using the ITE Toolbox\n",
    "\n",
    "* Author: J. Emmanuel Johnson\n",
    "* Email: jemanjohnson34@gmail.com\n",
    "* Date: $4^{\\text{th}}$ September, $2019$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walk-through how one can calculate a few key Information theory (IT) measures using the ITE toolbox. We have done previous experiments with the MATLAB package but there is a python version that can be useful for Python users. It's a lot cleaner but some of the functionality may be difficult to follow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "* Gael Implementation - [Gist](https://gist.github.com/GaelVaroquaux/ead9898bd3c973c40429)\n",
    "* ITE sub imples - [Github](https://github.com/aylliote/high_dimensional_mutual_information)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Literature Review (what we previous did)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Entropy\n",
    "\n",
    "In our experiments, we were only looking at Shannon entropy. It is the general case of Renyi's entropy as $\\alpha \\rightarrow 1$. We chose not to look at Renyi's entropy because we did not want to go down a rabbit hole of measures that we cannont understand nor justify. So we stuck to the basics. It's also important to keep in mind that we were looking at measures that could calculate the joint entropy; i.e. for multivariate, multi-dimensional datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithms\n",
    "\n",
    "##### KnnK\n",
    "\n",
    "This uses the KNN method to estimate the entropy. From what I understand, it's the simplest method that may have some issues at higher dimensions and large number of samples (normal with KNN estimators). In relation to the other standard methods of density estimation, it is the most robust in higher dimensions due to its adaptive-like binning.\n",
    "\n",
    "\n",
    "* A new class of random vector entropy estimators and its applications in testing statistical hypotheses - Goria et. al. (2005) - [Paper](https://www.tandfonline.com/doi/full/10.1080/104852504200026815)\n",
    "* Nearest neighbor estimates of entropy - Singh et. al. (2003) - [paper]()\n",
    "* A statistical estimate for the entropy of a random vector - Kozachenko et. al. (1987) - [paper]()\n",
    "\n",
    "##### KDP\n",
    "\n",
    "This is the logical progression from KnnK. It uses KD partitioning trees (KDTree) algorithm to speed up the calculations I presume.\n",
    "\n",
    "* Fast multidimensional entropy estimation by k-d partitioning - Stowell & Plumbley (2009) - [Paper]()\n",
    "\n",
    "##### expF \n",
    "\n",
    "This is the close-form expression for the Sharma-Mittal entropy calculation for expontial families. This estimates Y using the maximum likelihood estimation and then uses the analytical formula for the exponential family.\n",
    "\n",
    "* A closed-form expression for the Sharma-Mittal entropy of exponential families - Nielsen & Nock (2012) - [Paper]()\n",
    "\n",
    "##### vME\n",
    "\n",
    "This estimates the Shannon differential entropy (H) using the von Mises expansion. \n",
    "\n",
    "* Nonparametric von Mises estimators for entropies, divergences and mutual informations - Kandasamy et. al. (2015) - [Paper]()\n",
    "\n",
    "##### Ensemble\n",
    "\n",
    "Estimates the entropy from the average entropy estimations on groups of samples\n",
    "\n",
    "\n",
    "This is a simple implementation with the freedom to choose the estimator `estimate_H`.\n",
    "\n",
    "```python\n",
    "# split into groups\n",
    "for igroup in batches:\n",
    "    H += estimate_H(igroup)\n",
    "    \n",
    "H /= len(batches)\n",
    "```\n",
    "\n",
    "* High-dimensional mutual information estimation for image registration - Kybic (2004) - [Paper]()\n",
    "\n",
    "\n",
    "#### Potential New Experiments\n",
    "\n",
    "#### Voronoi\n",
    "\n",
    "Estimates Shannon entropy using Voronoi regions. Apparently it is good for multi-dimensional densities.\n",
    "\n",
    "* A new class of entropy estimators for multi-dimensional densities - Miller (2003) - [Paper]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "cwd = os.getcwd()\n",
    "sys.path.insert(0, f'{cwd}/../../src')\n",
    "sys.path.insert(0, f'{cwd}/../../src/itetoolbox')\n",
    "\n",
    "import numpy as np\n",
    "import ite\n",
    "# from data.load_TishbyData import load_TishbyData\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will simulate some data X that is normally distributed and Y which is X that has been rotated by some random matrix A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)    # reproducibility\n",
    "n_samples    = 1000\n",
    "d_dimensions = 3\n",
    "\n",
    "# create dataset X\n",
    "X = np.random.randn(n_samples, d_dimensions)\n",
    "\n",
    "# do some random rotation\n",
    "A = np.random.rand(d_dimensions, d_dimensions)\n",
    "\n",
    "# create dataset Y\n",
    "Y = X @ A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our experiments, we were only looking at Shannon entropy. It is the general case of Renyi's entropy as $\\alpha \\rightarrow 1$. We chose not to look at Renyi's entropy because we did not want to go down a rabbit hole of measures that we cannont understand nor justify. So we stuck to the basics. It's also important to keep in mind that we were looking at measures that could calculate the joint entropy; i.e. for multivariate, multi-dimensional datasets.\n",
    "\n",
    "\n",
    "## Algorithms\n",
    "\n",
    "### KnnK\n",
    "\n",
    "This uses the KNN method to estimate the entropy. From what I understand, it's the simplest method that may have some issues at higher dimensions and large number of samples (normal with KNN estimators). \n",
    "\n",
    "\n",
    "* A new class of random vector entropy estimators and its applications in testing statistical hypotheses - Goria et. al. (2005) - [Paper](https://www.tandfonline.com/doi/full/10.1080/104852504200026815)\n",
    "* Nearest neighbor estimates of entropy - Singh et. al. (2003) - [paper]()\n",
    "* A statistical estimate for the entropy of a random vector - Kozachenko et. al. (1987) - [paper]()\n",
    "\n",
    "This method works by calculating the nearest neighbors formula\n",
    "### KDP\n",
    "\n",
    "This is the logical progression from KnnK. It uses KD partitioning trees (KDTree) algorithm to speed up the calculations I presume.\n",
    "\n",
    "* Fast multidimensional entropy estimation by k-d partitioning - Stowell & Plumbley (2009) - [Paper]()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm\n",
    "\n",
    "1. Calculate the KNN Distances using the distance matrix\n",
    "2. Calculate the Volume of the unit ball wrt d_dimensions\n",
    "3. Calculate the entropy measure\n",
    "\n",
    "$$H = \\log (N - 1) - \\psi(k) + \\log (v) + D * \\frac{1}{N} \\sum \\log D$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_gaussian(C):\n",
    "    '''\n",
    "    Entropy of a gaussian variable with covariance matrix C\n",
    "    '''\n",
    "    if np.isscalar(C): # C is the variance\n",
    "        return .5*(1 + np.log(2*pi)) + .5*np.log(C)\n",
    "    else:\n",
    "        n = C.shape[0] # dimension\n",
    "        return .5*n*(1 + np.log(2*pi)) + .5*np.log(abs(det(C)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shannon Entropy (KNN/KDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gamma\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from typing import Optional\n",
    "\n",
    "# volume of unit ball\n",
    "def volume_unit_ball(d_dimensions: int)-> float:\n",
    "    \"\"\"Volume of the d-dimensional unit ball\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d_dimensions : int\n",
    "        Number of dimensions to estimate the volume\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    vol : float\n",
    "        The volume of the d-dimensional unit ball\n",
    "    \"\"\"\n",
    "    return ( np.pi**(.5 * d_dimensions) ) / gamma(.5 * d_dimensions + 1)\n",
    "\n",
    "\n",
    "# KNN Distances\n",
    "def knn_distance(X: np.ndarray, n_neighbors: int=20, algorithm: str='brute', n_jobs: int=-1, kwargs: Optional[dict]=None)-> np.ndarray:\n",
    "    \"\"\"Light wrapper around sklearn library.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, (n_samples x d_dimensions)\n",
    "        The data to find the nearest neighbors for.\n",
    "    \n",
    "    n_neighbors : int, default=20\n",
    "        The number of nearest neighbors to find.\n",
    "    \n",
    "    algorithm : str, default='brute', \n",
    "        The knn algorithm to use.\n",
    "        ('brute', 'ball_tree', 'kd_tree', 'auto')\n",
    "    \n",
    "    n_jobs : int, default=-1\n",
    "        The number of cores to use to find the nearest neighbors\n",
    "    \n",
    "    kwargs : dict, Optional\n",
    "        Any extra keyword arguments.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    distances : np.ndarray, (n_samples x d_dimensions)\n",
    "    \"\"\"\n",
    "    if kwargs:\n",
    "        clf_knn = NearestNeighbors(\n",
    "            n_neighbors=n_neighbors,\n",
    "            algorithm=algorithm,\n",
    "            n_jobs=n_jobs,\n",
    "            **kwargs\n",
    "        )\n",
    "    else:\n",
    "        clf_knn = NearestNeighbors(\n",
    "            n_neighbors=n_neighbors,\n",
    "            algorithm=algorithm,\n",
    "            n_jobs=n_jobs,\n",
    "        )\n",
    "\n",
    "    clf_knn.fit(X);\n",
    "\n",
    "    dists, _ = clf_knn.kneighbors(X)\n",
    "    \n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils import gen_batches\n",
    "\n",
    "class Ensemble:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def _fit_ensemble(self, X: np.ndarray, batch_size: int=100)-> float:\n",
    "        \n",
    "        Hs = list()\n",
    "        for idx in gen_batches(X.shape[0], batch_size, 10):\n",
    "            Hs.append(self._fit(X[idx]))\n",
    "        \n",
    "        \n",
    "        return np.mean(Hs)\n",
    "    \n",
    "class EntropyKNN(BaseEstimator, Ensemble):\n",
    "    def __init__(self, n_neighbors: int=20, algorithm: str='brute', n_jobs: int=-1, ensemble=False, batch_size=100, kwargs: Optional[dict]=None)-> None:\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.algorithm = algorithm\n",
    "        self.n_jobs = n_jobs\n",
    "        self.ensemble = ensemble\n",
    "        self.kwargs = kwargs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def fit(self, X: np.ndarray)-> BaseEstimator:\n",
    "        \n",
    "        self.vol = volume_unit_ball(X.shape[1])\n",
    "        \n",
    "        if self.ensemble:\n",
    "            self.H_x = self._fit_ensemble(X, self.batch_size)\n",
    "        else:\n",
    "            self.H_x = self._fit(X)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def _fit(self, X: np.ndarray)-> float:\n",
    "        \n",
    "        # 1. Calculate the K-nearest neighbors\n",
    "        dist = knn_distance(\n",
    "            X,\n",
    "            n_neighbors=self.n_neighbors,\n",
    "            algorithm=self.algorithm,\n",
    "            n_jobs=self.n_jobs,\n",
    "            kwargs=self.kwargs\n",
    "        )\n",
    "        \n",
    "        return np.log(n_samples - 1) - psi(n_neighbors) + np.log(self.vol) + ( d_dimensions / n_samples) * np.log(dist[:, n_neighbors-1]).sum()\n",
    "    \n",
    "    def score(self, X):\n",
    "        \n",
    "        return self.H_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X): 4.077 bits\n",
      "H(Y): 2.131 bits\n"
     ]
    }
   ],
   "source": [
    "# parameters (default)\n",
    "n_neighbors = 20\n",
    "algorithm = 'brute'\n",
    "n_jobs = -1\n",
    "ensemble = False\n",
    "batch_size = 50\n",
    "kwargs = {'metric': 'euclidean'}\n",
    "\n",
    "# initialize it estimator\n",
    "clf_knnK = EntropyKNN(\n",
    "    n_neighbors=n_neighbors,\n",
    "    algorithm=algorithm,\n",
    "    n_jobs=n_jobs,\n",
    "    ensemble=ensemble,\n",
    "    batch_size=batch_size,\n",
    "    kwargs=kwargs,\n",
    "    \n",
    ")\n",
    "\n",
    "# estimate entropy\n",
    "H_x = clf_knnK.fit(X).score(X)\n",
    "H_y = clf_knnK.fit(Y).score(Y)\n",
    "\n",
    "print(f\"H(X): {H_x:.3f} bits\")\n",
    "print(f\"H(Y): {H_y:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice there are quite a lot of parameters we can change within the actual KNN estimation procedure. But the rest seems to be fairly consistent with not much tweaking we can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ITE Toolbox implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X): 4.132 bits\n",
      "H(Y): 2.208 bits\n"
     ]
    }
   ],
   "source": [
    "# parameters (default)\n",
    "mult        = True\n",
    "knn_method  = 'cKDTree'      # fast version (slower version KNN)\n",
    "k_neighbors = 10             # free parameter\n",
    "eps         = 0.1            # free parameter\n",
    "\n",
    "# initialize it estimator\n",
    "clf_knnK = ite.cost.BHShannon_KnnK(\n",
    "    mult=mult, \n",
    "    knn_method=knn_method,\n",
    "    k=k_neighbors,\n",
    "    eps=eps\n",
    ")\n",
    "\n",
    "# estimate entropy\n",
    "H_x = clf_knnK.estimation(X)\n",
    "H_y = clf_knnK.estimation(Y)\n",
    "\n",
    "print(f\"H(X): {H_x:.3f} bits\")\n",
    "print(f\"H(Y): {H_y:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the numbers we get are quite similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon Entropy: expF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the close-form expression for the Sharma-Mittal entropy calculation for expontial families. The Sharma-Mittal entropy is a generalization of the Shannon, RÃ©nyi and Tsallis entropy measurements. This estimates Y using the maximum likelihood estimation and then uses the analytical formula for the exponential family.\n",
    "\n",
    "* A closed-form expression for the Sharma-Mittal entropy of exponential families - Nielsen & Nock (2012) - [Paper]()\n",
    "* Statistical exponential families: A digest with flash cards - [Paper](https://arxiv.org/pdf/0911.4863.pdf)\n",
    "\n",
    "\n",
    "**Source Parameters**\n",
    "\n",
    "$$\\Lambda = (\\mu, \\Sigma)$$\n",
    "\n",
    "where $\\mu \\in \\mathbb{R}^{d}$ and $\\Sigma > 0$\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "$$\\Theta = \\left( \\Sigma^{-1}\\mu, \\frac{1}{2}\\Sigma^{-1} \\right)$$\n",
    "\n",
    "**Log Normalizer**\n",
    "\n",
    "$$F(\\Theta) = \\frac{1}{4} Tr( \\theta^\\top \\Theta^{-1} \\theta) - \\frac{1}{2} \\log|\\Theta| + \\frac{d}{2}\\log \\pi$$\n",
    "\n",
    "**Gradient Log Normalizer**\n",
    "\n",
    "$$\\nabla F(\\Theta) = \\left( \\frac{1}{2} \\Theta^{-1}\\theta, -\\frac{1}{2} \\Theta^{-1}- \\frac{1}{4}(\\Theta^{-1}\\Theta)(\\Theta^{-1}\\Theta)^\\top \\right)$$\n",
    "\n",
    "**Final Entropy Calculation**\n",
    "\n",
    "$$H = F(\\Theta) - \\langle F(\\Theta), \\nabla F(\\Theta) \\rangle$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expF_entropy(X):\n",
    "    \n",
    "    # estimate Gaussian parameters\n",
    "    mean = X.mean(axis=0)\n",
    "    cov = np.cov(X.T)\n",
    "    \n",
    "    # make Gaussian distribution\n",
    "    norm_dist = stats.multivariate_normal(mean=mean, cov=cov, seed=seed)\n",
    "    \n",
    "    # estimate the entropy from closed form solution\n",
    "    H_x = norm_dist.entropy()\n",
    "    \n",
    "    return H_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X): 4.195 bits\n",
      "H(Y): 1.329 bits\n"
     ]
    }
   ],
   "source": [
    "H_x = expF_entropy(X)\n",
    "H_y = expF_entropy(Y)\n",
    "\n",
    "print(f\"H(X): {H_x:.3f} bits\")\n",
    "print(f\"H(Y): {H_y:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it works well if the distribution is actuall Gaussian but it doesn't if it isn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X): 4.195 bits\n"
     ]
    }
   ],
   "source": [
    "mean = X.mean(axis=0)\n",
    "cov = np.cov(X.T)\n",
    "seed = 1\n",
    "\n",
    "norm_dist = stats.multivariate_normal(mean=mean, cov=cov, seed=seed)\n",
    "\n",
    "H_x = norm_dist.entropy()\n",
    "\n",
    "print(f\"H(X): {H_x:.3f} bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-79-af7a3dc7f9a4>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-79-af7a3dc7f9a4>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    alpha_grad =\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 1. estimate the maximum likelihood params\n",
    "mean = X.mean(axis=0)[:, None]\n",
    "cov = np.cov(X.T)\n",
    "inv_cov = np.linalg.inv(cov)\n",
    "alpha = inv_cov @ mean\n",
    "sigma = inv_cov / 2\n",
    "\n",
    "mean.shape, cov.shape, inv_cov.shape, t1.shape, t2.shape\n",
    "\n",
    "# Log Normalizer (Maximum Like)\n",
    "F = (1/4) * np.trace(np.linalg.inv(t2) @ t1 @ t1.T) - (1/2) * np.log(np.linalg.det(cov)) + (X.shape[1] / 2) * np.log(np.pi)\n",
    "\n",
    "# Gradient Log Normalizer\n",
    "alpha_grad = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3,), (3, 3), (3, 3), (3,), (3, 3))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean.shape, cov.shape, inv_cov.shape, t1.shape, t2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### vME\n",
    "\n",
    "This nonparametric method that estimates the Shannon differential entropy (H) using the von Mises expansion. This method has a fast convergence rate than the KDE and KNN methods. This algorithm does have and in addition the can be tuned using cross-validation techniques. It is also less expensive than the KDE in terms of the numerical integration whereas this method has closed form solutions for some families of von Mises expansions.\n",
    "\n",
    "* Nonparametric von Mises estimators for entropies, divergences and mutual informations - Kandasamy et. al. (2015) - [Paper]()\n",
    "\n",
    "##### Ensemble\n",
    "\n",
    "Estimates the entropy from the average entropy estimations on groups of samples\n",
    "\n",
    "\n",
    "This is a simple implementation with the freedom to choose the estimator `estimate_H`.\n",
    "\n",
    "```python\n",
    "# split into groups\n",
    "for igroup in batches:\n",
    "    H += estimate_H(igroup)\n",
    "    \n",
    "H /= len(batches)\n",
    "```\n",
    "\n",
    "* High-dimensional mutual information estimation for image registration - Kybic (2004) - [Paper]()\n",
    "\n",
    "\n",
    "#### Potential New Experiments\n",
    "\n",
    "#### Voronoi\n",
    "\n",
    "Estimates Shannon entropy using Voronoi regions. Apparently it is good for multi-dimensional densities.\n",
    "\n",
    "* A new class of entropy estimators for multi-dimensional densities - Miller (2003) - [Paper]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimation was carried out using the following relationship. Let $XY = [X, Y] \\in \\mathcal{R}^{N \\times D}$, where $D=D_1+D_2$.\n",
    "\n",
    "$$I(XY) = \\sum_{d=1}^D H(XY) - H(XY)$$\n",
    "\n",
    "The pseudo-code is fairly simple (in the MATLAB version).\n",
    "\n",
    "\n",
    "1. Organize the components\n",
    "\n",
    "```python\n",
    "XY = [X, Y]\n",
    "```\n",
    "\n",
    "2. Estimate the joint entropy, $H(XY)$\n",
    "\n",
    "```python\n",
    "H_xy = - estimate_H(\n",
    "    np.hstack(XY)     # stack the vectors dimension-wise\n",
    ")\n",
    "```\n",
    "\n",
    "3. Estimate the marginals of XY; i.e. estimate X and Y individually, then sum them.\n",
    "```python\n",
    "H_x_y = np.sum(\n",
    "    # estimate the entropy for each marginal\n",
    "    [estimate_H(imarginal) for imarginal in XY]\n",
    ")\n",
    "```\n",
    "\n",
    "4. Summation of the two quantities\n",
    "\n",
    "```python\n",
    "MI_XY = H_x_y + H_xy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble:\n",
    "    def _fit_ensemble(self, X: np.ndarray, vol: float, batch_size: int=100)-> float:\n",
    "        \n",
    "        Hs = list()\n",
    "        for idx in gen_batches(X.shape[0], batch_size, 10):\n",
    "            Hs.append(self._fit(X[idx], vol))\n",
    "        \n",
    "        \n",
    "        return np.mean(Hs)\n",
    "\n",
    "class MutualInfoKNN(BaseEstimator):\n",
    "    def __init__(self, n_neighbors: int=20, algorithm: str='brute', n_jobs: int=-1, kwargs: Optional[dict]=None)-> None:\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.algorithm = algorithm\n",
    "        self.n_jobs = n_jobs\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray)-> BaseEstimator:\n",
    "        \n",
    "        # Calculate Volumes\n",
    "        vol_xy = volume_unit_ball(X.shape[1] + Y.shape[1])\n",
    "        vol_x = volume_unit_ball(X.shape[1])\n",
    "        vol_y = volume_unit_ball(Y.shape[1])\n",
    "        \n",
    "        # Calculate Joint Entropy\n",
    "        H_xy = self._fit(np.vstack([X, Y]), vol_xy)\n",
    "        \n",
    "        # Calculate Marginal Probabilities\n",
    "        H_x = self._fit(X, vol_x)\n",
    "        H_y = self._fit(Y, vol_y)\n",
    "            \n",
    "        # Calculate Mutual Information\n",
    "        self.MI = H_x + H_y - H_xy\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def _fit(self, X: np.ndarray, vol: float)-> float:\n",
    "        \n",
    "        # 1. Calculate the K-nearest neighbors\n",
    "        dist = knn_distance(\n",
    "            X,\n",
    "            n_neighbors=self.n_neighbors,\n",
    "            algorithm=self.algorithm,\n",
    "            n_jobs=self.n_jobs,\n",
    "            kwargs=self.kwargs\n",
    "        )\n",
    "        \n",
    "        return np.log(n_samples - 1) - psi(n_neighbors) + np.log(vol) + ( d_dimensions / n_samples) * np.log(dist[:, n_neighbors-1]).sum()\n",
    "    \n",
    "    def score(self, X):\n",
    "        \n",
    "        return self.MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X): 6.427 bits\n"
     ]
    }
   ],
   "source": [
    "# parameters (default)\n",
    "n_neighbors = 10\n",
    "algorithm = 'brute'\n",
    "n_jobs = -1\n",
    "ensemble = False\n",
    "batch_size = 50\n",
    "kwargs = {'metric': 'euclidean'}\n",
    "\n",
    "# initialize it estimator\n",
    "clf_knnK = MutualInfoKNN(\n",
    "    n_neighbors=n_neighbors,\n",
    "    algorithm=algorithm,\n",
    "    n_jobs=n_jobs,\n",
    "    kwargs=kwargs,\n",
    "    \n",
    ")\n",
    "\n",
    "# estimate entropy\n",
    "MI_xy = clf_knnK.fit(X, Y).score(X)\n",
    "\n",
    "print(f\"H(X): {MI_xy:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ITE Toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MI(X,Y): 3.683 bits\n"
     ]
    }
   ],
   "source": [
    "# parameters (default)\n",
    "mult       = True          # ??\n",
    "kl_co_name = 'BDKL_KnnK'   # KLD calculator\n",
    "kl_co_pars = None          # parameters for the KLD calculator\n",
    "\n",
    "# initialize it estimator\n",
    "clf_mi = ite.cost.MIShannon_DKL(\n",
    "#     mult=mult,\n",
    "#     kl_co_name=kl_co_name,\n",
    "#     kl_co_pars=kl_co_pars,\n",
    ")\n",
    "\n",
    "# concat data\n",
    "XY = np.concatenate((X, Y), axis=1)\n",
    "\n",
    "# individual dimensions per\n",
    "sub_dimensions = np.array([X.shape[1], Y.shape[1]])\n",
    "\n",
    "# estimate mutual information\n",
    "mi_XY = clf_mi.estimation(XY, sub_dimensions)\n",
    "\n",
    "print(f\"MI(X,Y): {mi_XY:.3f} bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MI(X,Y): 2.062 bits\n"
     ]
    }
   ],
   "source": [
    "# parameters (default)\n",
    "mult       = True          # ??\n",
    "kernel = {'name': 'RBF','sigma': 1}   # KLD calculator\n",
    "eta  = 0.01          # parameters for the KLD calculator\n",
    "\n",
    "# initialize it estimator\n",
    "clf_mi = ite.cost.BIKGV(\n",
    "#     mult=mult,\n",
    "#     kernel=kernel,\n",
    "#     eta=eta,\n",
    ")\n",
    "\n",
    "# concat data\n",
    "XY = np.concatenate((X, Y), axis=1)\n",
    "\n",
    "# individual dimensions per\n",
    "sub_dimensions = np.array([X.shape[1], Y.shape[1]])\n",
    "\n",
    "# estimate mutual information\n",
    "mi_XY = clf_mi.estimation(XY, sub_dimensions)\n",
    "\n",
    "print(f\"MI(X,Y): {mi_XY:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I expect there to be some MI between X and Y since it is a rotation of the original distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "---\n",
    "### Total Correlation (Multi-Information, Co-Information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimation was carried out using the following relationship:\n",
    "\n",
    "$$I(x_1, x_2, \\ldots, x_D) = \\sum_{d=1}^D H(x_d) - H(X)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon Total Correlation, TC(X): -0.002 bits\n",
      "Shannon Total Correlation, TC(Y): 2.365 bits\n"
     ]
    }
   ],
   "source": [
    "# parameters (default)\n",
    "mult       = True\n",
    "kl_co_name = 'BDKL_KnnK'\n",
    "kl_co_pars = None\n",
    "\n",
    "# initialize it estimator\n",
    "clf_mi = ite.cost.MIShannon_DKL(\n",
    "    mult=mult,\n",
    "    kl_co_name=kl_co_name,\n",
    "    kl_co_pars=kl_co_pars,\n",
    ")\n",
    "\n",
    "# concat data\n",
    "sub_dimensions = np.array(range(X.shape[1]))\n",
    "\n",
    "# estimate mutual information\n",
    "tc_X = clf_mi.estimation(X, sub_dimensions)\n",
    "tc_Y = clf_mi.estimation(Y, sub_dimensions)\n",
    "\n",
    "print(f\"Shannon Total Correlation, TC(X): {tc_X:.3f} bits\")\n",
    "print(f\"Shannon Total Correlation, TC(Y): {tc_Y:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes since given that the original distribution $X$ should have no correlations between dimensions because it is Gaussian. The rotation of $X$ by some random matrix $A$, $Y=AX^{\\top}$, means that we have added some correlations between dimensions. We see that as the TC is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-it4dnn]",
   "language": "python",
   "name": "conda-env-.conda-it4dnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
