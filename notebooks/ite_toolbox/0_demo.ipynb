{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Information Theory Measures using the ITE Toolbox\n",
    "\n",
    "* Author: J. Emmanuel Johnson\n",
    "* Email: jemanjohnson34@gmail.com\n",
    "* Date: $4^{\\text{th}}$ September, $2019$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walk-through how one can calculate a few key Information theory (IT) measures using the ITE toolbox. We have done previous experiments with the MATLAB package but there is a python version that can be useful for Python users. It's a lot cleaner but some of the functionality may be difficult to follow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Literature Review (what we previous did)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Entropy\n",
    "\n",
    "In our experiments, we were only looking at Shannon entropy. It is the general case of Renyi's entropy as $\\alpha \\rightarrow 1$. We chose not to look at Renyi's entropy because we did not want to go down a rabbit hole of measures that we cannont understand nor justify. So we stuck to the basics. It's also important to keep in mind that we were looking at measures that could calculate the joint entropy; i.e. for multivariate, multi-dimensional datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithms\n",
    "\n",
    "##### KnnK\n",
    "\n",
    "This uses the KNN method to estimate the entropy. From what I understand, it's the simplest method that may have some issues at higher dimensions and large number of samples (normal with KNN estimators). \n",
    "\n",
    "\n",
    "* A new class of random vector entropy estimators and its applications in testing statistical hypotheses - Goria et. al. (2005) - [Paper](https://www.tandfonline.com/doi/full/10.1080/104852504200026815)\n",
    "* Nearest neighbor estimates of entropy - Singh et. al. (2003) - [paper]()\n",
    "* A statistical estimate for the entropy of a random vector - Kozachenko et. al. (1987) - [paper]()\n",
    "\n",
    "##### KDP\n",
    "\n",
    "This is the logical progression from KnnK. It uses KD partitioning trees (KDTree) algorithm to speed up the calculations I presume.\n",
    "\n",
    "* Fast multidimensional entropy estimation by k-d partitioning - Stowell & Plumbley (2009) - [Paper]()\n",
    "\n",
    "##### expF \n",
    "\n",
    "This is the close-form expression for the Sharma-Mittal entropy calculation for expontial families. This estimates Y using the maximum likelihood estimation and then uses the analytical formula for the exponential family.\n",
    "\n",
    "* A closed-form expression for the Sharma-Mittal entropy of exponential families - Nielsen & Nock (2012) - [Paper]()\n",
    "\n",
    "##### vME\n",
    "\n",
    "This estimates the Shannon differential entropy (H) using the von Mises expansion. \n",
    "\n",
    "* Nonparametric von Mises estimators for entropies, divergences and mutual informations - Kandasamy et. al. (2015) - [Paper]()\n",
    "\n",
    "##### Ensemble\n",
    "\n",
    "Estimates the entropy from the average entropy estimations on groups of samples\n",
    "\n",
    "\n",
    "This is a simple implementation with the freedom to choose the estimator `estimate_H`.\n",
    "\n",
    "```python\n",
    "# split into groups\n",
    "for igroup in batches:\n",
    "    H += estimate_H(igroup)\n",
    "    \n",
    "H /= len(batches)\n",
    "```\n",
    "\n",
    "* High-dimensional mutual information estimation for image registration - Kybic (2004) - [Paper]()\n",
    "\n",
    "\n",
    "#### Potential New Experiments\n",
    "\n",
    "#### Voronoi\n",
    "\n",
    "Estimates Shannon entropy using Voronoi regions. Apparently it is good for multi-dimensional densities.\n",
    "\n",
    "* A new class of entropy estimators for multi-dimensional densities - Miller (2003) - [Paper]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "cwd = os.getcwd()\n",
    "sys.path.insert(0, f'{cwd}/../../src')\n",
    "sys.path.insert(0, f'{cwd}/../../src/itetoolbox')\n",
    "\n",
    "import numpy as np\n",
    "import ite\n",
    "from data.load_TishbyData import load_TishbyData\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will simulate some data X that is normally distributed and Y which is X that has been rotated by some random matrix A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)    # reproducibility\n",
    "n_samples    = 1000\n",
    "d_dimensions = 3\n",
    "\n",
    "# create dataset X\n",
    "X = np.random.randn(n_samples, d_dimensions)\n",
    "\n",
    "# do some random rotation\n",
    "A = np.random.rand(d_dimensions, d_dimensions)\n",
    "\n",
    "# create dataset Y\n",
    "Y = X @ A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shannon Entropy (KNN/KDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X): 4.132 bits\n",
      "H(Y): 2.208 bits\n"
     ]
    }
   ],
   "source": [
    "# parameters (default)\n",
    "mult        = True\n",
    "knn_method  = 'cKDTree'      # fast version (slower version KNN)\n",
    "k_neighbors = 20             # free parameter\n",
    "eps         = 0.1            # free parameter\n",
    "\n",
    "# initialize it estimator\n",
    "clf_knnK = ite.cost.BHShannon_KnnK(\n",
    "    mult=mult, \n",
    "    knn_method=knn_method,\n",
    "    k=k_neighbors,\n",
    "    eps=eps\n",
    ")\n",
    "\n",
    "# estimate entropy\n",
    "H_x = clf_knnK.estimation(X)\n",
    "H_y = clf_knnK.estimation(Y)\n",
    "\n",
    "print(f\"H(X): {H_x:.3f} bits\")\n",
    "print(f\"H(Y): {H_y:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimation was carried out using the following relationship. Let $XY = [X, Y] \\in \\mathcal{R}^{N \\times D}$, where $D=D_1+D_2$.\n",
    "\n",
    "$$I(XY) = \\sum_{d=1}^D H(XY) - H(XY)$$\n",
    "\n",
    "The pseudo-code is fairly simple (in the MATLAB version).\n",
    "\n",
    "\n",
    "1. Organize the components\n",
    "\n",
    "```python\n",
    "XY = [X, Y]\n",
    "```\n",
    "\n",
    "2. Estimate the joint entropy, $H(XY)$\n",
    "\n",
    "```python\n",
    "H_xy = - estimate_H(\n",
    "    np.hstack(XY)     # stack the vectors dimension-wise\n",
    ")\n",
    "```\n",
    "\n",
    "3. Estimate the marginals of XY; i.e. estimate X and Y individually, then sum them.\n",
    "```python\n",
    "H_x_y = np.sum(\n",
    "    # estimate the entropy for each marginal\n",
    "    [estimate_H(imarginal) for imarginal in XY]\n",
    ")\n",
    "```\n",
    "\n",
    "4. Summation of the two quantities\n",
    "\n",
    "```python\n",
    "MI_XY = H_x_y + H_xy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MI(X,Y): 3.584 bits\n"
     ]
    }
   ],
   "source": [
    "# parameters (default)\n",
    "mult       = True          # ??\n",
    "kl_co_name = 'BDKL_KnnK'   # KLD calculator\n",
    "kl_co_pars = None          # parameters for the KLD calculator\n",
    "\n",
    "# initialize it estimator\n",
    "clf_mi = ite.cost.MIShannon_DKL(\n",
    "    mult=mult,\n",
    "    kl_co_name=kl_co_name,\n",
    "    kl_co_pars=kl_co_pars,\n",
    ")\n",
    "\n",
    "# concat data\n",
    "XY = np.concatenate((X, Y), axis=1)\n",
    "\n",
    "# individual dimensions per\n",
    "sub_dimensions = np.array([X.shape[1], Y.shape[1]])\n",
    "\n",
    "# estimate mutual information\n",
    "mi_XY = clf_mi.estimation(XY, sub_dimensions)\n",
    "\n",
    "print(f\"MI(X,Y): {mi_XY:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I expect there to be some MI between X and Y since it is a rotation of the original distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "---\n",
    "### Total Correlation (Multi-Information, Co-Information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimation was carried out using the following relationship:\n",
    "\n",
    "$$I(x_1, x_2, \\ldots, x_D) = \\sum_{d=1}^D H(x_d) - H(X)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon Total Correlation, TC(X): -0.002 bits\n",
      "Shannon Total Correlation, TC(Y): 2.365 bits\n"
     ]
    }
   ],
   "source": [
    "# parameters (default)\n",
    "mult       = True\n",
    "kl_co_name = 'BDKL_KnnK'\n",
    "kl_co_pars = None\n",
    "\n",
    "# initialize it estimator\n",
    "clf_mi = ite.cost.MIShannon_DKL(\n",
    "    mult=mult,\n",
    "    kl_co_name=kl_co_name,\n",
    "    kl_co_pars=kl_co_pars,\n",
    ")\n",
    "\n",
    "# concat data\n",
    "sub_dimensions = np.array(range(X.shape[1]))\n",
    "\n",
    "# estimate mutual information\n",
    "tc_X = clf_mi.estimation(X, sub_dimensions)\n",
    "tc_Y = clf_mi.estimation(Y, sub_dimensions)\n",
    "\n",
    "print(f\"Shannon Total Correlation, TC(X): {tc_X:.3f} bits\")\n",
    "print(f\"Shannon Total Correlation, TC(Y): {tc_Y:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes since given that the original distribution $X$ should have no correlations between dimensions because it is Gaussian. The rotation of $X$ by some random matrix $A$, $Y=AX^{\\top}$, means that we have added some correlations between dimensions. We see that as the TC is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-it4dnn]",
   "language": "python",
   "name": "conda-env-.conda-it4dnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
